"""Core chat orchestration service â€” the 9-step pipeline."""

import logging
from typing import AsyncGenerator

from sqlalchemy.ext.asyncio import AsyncSession

from app.models.user import User
from app.models.conversation import Conversation
from app.models.message import Message
from app.services.sensitive_service import check_sensitive
from app.services.risk_service import classify_risk
from app.services.emotion_service import detect_emotion
from app.services.calendar_service import get_current_tone
from app.services.knowledge_service import search as knowledge_search, format_sources_for_prompt, format_sources_for_citation
from app.services.llm_service import llm_router

logger = logging.getLogger(__name__)

# Role-specific system prompt fragments
ROLE_PROMPTS = {
    "gaokao": "ç”¨æˆ·æ˜¯ä¸€åé«˜è€ƒè€ƒç”Ÿã€‚è¯·ç”¨é¼“åŠ±ã€å¹³æ˜“è¿‘äººçš„å£å»å›ç­”ï¼Œä¾§é‡æœ¬ç§‘æ‹›ç”Ÿä¿¡æ¯ã€‚",
    "kaoyan": "ç”¨æˆ·æ˜¯ä¸€åè€ƒç ”å­¦ç”Ÿã€‚è¯·ç”¨ä¸“ä¸šã€è¯¦ç»†çš„å£å»å›ç­”ï¼Œä¾§é‡ç ”ç©¶ç”Ÿæ‹›ç”Ÿå’Œå­¦æœ¯æ–¹å‘ã€‚",
    "international": "ç”¨æˆ·æ˜¯ä¸€åå›½é™…å­¦ç”Ÿã€‚è¯·ç”¨æ¸…æ™°ã€å‹å¥½çš„å£å»å›ç­”ï¼Œå¦‚æœ‰éœ€è¦å¯ç”¨ä¸­è‹±åŒè¯­ï¼Œä¾§é‡å›½é™…æ‹›ç”Ÿæ”¿ç­–å’Œç•™å­¦ç”Ÿæ”¯æŒã€‚",
    "parent": "ç”¨æˆ·æ˜¯ä¸€åè€ƒç”Ÿå®¶é•¿ã€‚è¯·ç”¨è€å¿ƒã€æ¸©å’Œã€è¯¦ç»†çš„å£å»å›ç­”ï¼Œä¾§é‡å®¶é•¿å…³å¿ƒçš„å°±ä¸šå‰æ™¯ã€æ ¡å›­å®‰å…¨å’Œå­¦è´¹ã€‚",
}

BASE_SYSTEM_PROMPT = """ä½ æ˜¯åŒ—äº¬å¸ˆèŒƒå¤§å­¦æ‹›ç”Ÿæ™ºèƒ½åŠ©æ‰‹"äº¬å¸ˆå°æ™º"ã€‚ä½ çš„èŒè´£æ˜¯åŸºäºåŒ—äº¬å¸ˆèŒƒå¤§å­¦å®˜æ–¹èµ„æ–™ï¼Œä¸ºè€ƒç”Ÿå’Œå®¶é•¿æä¾›å‡†ç¡®ã€å‹å¥½çš„æ‹›ç”Ÿå’¨è¯¢æœåŠ¡ã€‚

æ ¸å¿ƒè§„åˆ™ï¼š
1. æ‰€æœ‰å›ç­”å¿…é¡»åŸºäºçŸ¥è¯†åº“ä¸­çš„å®˜æ–¹èµ„æ–™ï¼Œä¸å¾—ç¼–é€ ä¿¡æ¯
2. æ¶‰åŠå…·ä½“æ•°å­—ï¼ˆåˆ†æ•°çº¿ã€å­¦è´¹ã€æ‹›ç”Ÿäººæ•°ç­‰ï¼‰æ—¶å¿…é¡»å¼•ç”¨æ¥æº
3. ä¸ç¡®å®šçš„ä¿¡æ¯è¯·å»ºè®®ç”¨æˆ·è”ç³»æ‹›ç”ŸåŠï¼ˆç”µè¯ï¼š010-58807962ï¼‰
4. ä¿æŒåŒ—äº¬å¸ˆèŒƒå¤§å­¦"å­¦ä¸ºäººå¸ˆï¼Œè¡Œä¸ºä¸–èŒƒ"çš„æ ¡è®­ç²¾ç¥
5. ä¸¥ç¦åšå‡ºä»»ä½•å½•å–æ‰¿è¯ºæˆ–ä¿è¯"""

HIGH_RISK_RESPONSE = "è¿™ä¸ªé—®é¢˜æ¶‰åŠå…·ä½“çš„æ‹›ç”Ÿæ”¿ç­–å’Œå½•å–æ ‡å‡†ï¼Œä¸ºç¡®ä¿ä¿¡æ¯å‡†ç¡®ï¼Œå»ºè®®æ‚¨ç›´æ¥è”ç³»åŒ—äº¬å¸ˆèŒƒå¤§å­¦æ‹›ç”ŸåŠï¼š\n\nğŸ“ ç”µè¯ï¼š010-58807962\nğŸŒ å®˜ç½‘ï¼šadmission.bnu.edu.cn\n\næ‹›ç”Ÿè€å¸ˆä¼šä¸ºæ‚¨æä¾›æœ€æƒå¨çš„è§£ç­”ã€‚"


async def process_message(
    user: User,
    conversation: Conversation,
    user_message: str,
    user_role: str | None,
    db: AsyncSession,
) -> AsyncGenerator[dict, None]:
    """Process a user message through the full pipeline, yielding streaming events.

    Yields dicts with type:
    - {"type": "sensitive_block", "content": str} â€” message blocked
    - {"type": "high_risk", "content": str} â€” high risk redirect
    - {"type": "token", "content": str} â€” streaming token
    - {"type": "done", "sources": list, "risk_level": str, "review_passed": bool}
    """

    # Step 1: Sensitive word pre-filter
    filter_result = await check_sensitive(user_message, db)
    if filter_result.action == "block":
        # Save blocked message
        msg = Message(
            conversation_id=conversation.id,
            role="user",
            content=user_message,
            risk_level="blocked",
        )
        db.add(msg)
        assistant_msg = Message(
            conversation_id=conversation.id,
            role="assistant",
            content=filter_result.message,
            risk_level="blocked",
        )
        db.add(assistant_msg)
        await db.commit()
        yield {"type": "sensitive_block", "content": filter_result.message}
        return

    # Step 2: Risk classification
    risk_level = classify_risk(user_message)

    if risk_level == "high":
        msg = Message(conversation_id=conversation.id, role="user", content=user_message, risk_level="high")
        db.add(msg)
        assistant_msg = Message(
            conversation_id=conversation.id, role="assistant",
            content=HIGH_RISK_RESPONSE, risk_level="high",
        )
        db.add(assistant_msg)
        await db.commit()
        yield {"type": "high_risk", "content": HIGH_RISK_RESPONSE}
        return

    # Step 3: Time-aware tone injection
    tone_config = await get_current_tone(db)
    tone_hint = tone_config.get("system_hint", "")

    # Step 4: Emotion detection
    emotion = detect_emotion(user_message)
    emotion_hint = ""
    if emotion.comfort_prefix:
        emotion_hint = f"\nç”¨æˆ·å¯èƒ½æ„Ÿåˆ°{emotion.emotion}ï¼Œè¯·åœ¨å›ç­”å¼€å¤´é€‚å½“åŠ å…¥å®‰æ…°å’Œé¼“åŠ±ã€‚"

    # Step 5: Knowledge base search
    search_results = await knowledge_search(user_message, db, top_k=5)
    knowledge_context = format_sources_for_prompt(search_results)
    sources_citation = format_sources_for_citation(search_results)

    # Step 6: Prompt assembly
    role_hint = ROLE_PROMPTS.get(user_role or "", "")
    citation_hint = ""
    if risk_level == "medium":
        citation_hint = '\né‡è¦ï¼šæœ¬æ¬¡å›ç­”å¿…é¡»å¼•ç”¨çŸ¥è¯†åº“æ¥æºï¼Œä½¿ç”¨"æ ¹æ®ã€Šxxxã€‹â€¦"æ ¼å¼ã€‚'

    system_prompt = f"{BASE_SYSTEM_PROMPT}\n\n{role_hint}\n{tone_hint}\n{emotion_hint}\n{citation_hint}"

    if knowledge_context:
        system_prompt += f"\n\nä»¥ä¸‹æ˜¯ç›¸å…³çŸ¥è¯†åº“å†…å®¹ï¼Œè¯·åŸºäºè¿™äº›å†…å®¹å›ç­”ï¼š\n\n{knowledge_context}"

    # Build message history (last 10 messages from conversation)
    messages = [{"role": "system", "content": system_prompt}]

    # Add recent conversation history
    for msg in (conversation.messages or [])[-10:]:
        if not msg.is_deleted:
            messages.append({"role": msg.role, "content": msg.content})

    messages.append({"role": "user", "content": user_message})

    # Save user message
    user_msg = Message(
        conversation_id=conversation.id,
        role="user",
        content=user_message,
        risk_level=risk_level,
    )
    db.add(user_msg)

    # Step 7: LLM streaming call
    full_response = []
    try:
        stream = await llm_router.chat(messages, stream=True)
        async for token in stream:
            full_response.append(token)
            yield {"type": "token", "content": token}
    except Exception as e:
        logger.error("LLM call failed: %s", e)
        error_msg = "æŠ±æ­‰ï¼Œç³»ç»Ÿæš‚æ—¶æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚"
        full_response = [error_msg]
        yield {"type": "token", "content": error_msg}

    response_text = "".join(full_response)

    # Step 8: Dual-model review (async â€” simplified inline for now)
    review_passed = True
    # In production, this would be dispatched as a Celery task

    # Step 9: Persist assistant message
    assistant_msg = Message(
        conversation_id=conversation.id,
        role="assistant",
        content=response_text,
        risk_level=risk_level,
        review_passed=review_passed,
        sources=sources_citation if sources_citation else None,
    )
    db.add(assistant_msg)
    await db.commit()

    yield {
        "type": "done",
        "sources": sources_citation,
        "risk_level": risk_level,
        "review_passed": review_passed,
    }
